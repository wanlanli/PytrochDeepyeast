{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecc00863-28b4-406c-bf78-5610ea1c9bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, distutils.core\n",
    "sys.path.insert(0, os.path.abspath('../../detectron2'))\n",
    "sys.path.insert(0, os.path.abspath('../../'))\n",
    "sys.path.insert(0, os.path.abspath('../projects/Panoptic-DeepLab/'))\n",
    "os.environ['DETECTRON2_DATASETS'] = '/home/liwa/data/datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b8329c0-02dd-47bf-a1ea-a606c4ebfdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liwa/miniconda3/envs/oneformer/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "# from oneformer.data.datasets.register_yeast_panoptic_annos_semseg import register_all_yeast_panoptic_annos_sem_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9360f2f-b410-4b84-a18d-31d644374674",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.abspath(\"./projects/Panoptic-DeepLab/configs/yeast_panoptics/city_deeplab_crop_512x512_300data.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c5db10-a1e5-4efb-a1b4-d30e9d66e655",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"train_net_yeast.py\", line 13, in <module>\n",
      "    import detectron2.data.transforms as T\n",
      "ModuleNotFoundError: No module named 'detectron2'\n"
     ]
    }
   ],
   "source": [
    "!python train_net_yeast.py --dist-url 'tcp://127.0.0.1:50164' \\\n",
    "--config-file $config_path \\\n",
    "--num-gpus 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97726f6d-81a2-42c7-9aff-592f69e873d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetCatalog(registered datasets: coco_2014_train, coco_2014_val, coco_2014_minival, coco_2014_valminusminival, coco_2017_train, coco_2017_val, coco_2017_test, coco_2017_test-dev, coco_2017_val_100, keypoints_coco_2014_train, keypoints_coco_2014_val, keypoints_coco_2014_minival, keypoints_coco_2014_valminusminival, keypoints_coco_2017_train, keypoints_coco_2017_val, keypoints_coco_2017_val_100, coco_2017_train_panoptic_separated, coco_2017_train_panoptic_stuffonly, coco_2017_train_panoptic, coco_2017_val_panoptic_separated, coco_2017_val_panoptic_stuffonly, coco_2017_val_panoptic, coco_2017_val_100_panoptic_separated, coco_2017_val_100_panoptic_stuffonly, coco_2017_val_100_panoptic, lvis_v1_train, lvis_v1_val, lvis_v1_test_dev, lvis_v1_test_challenge, lvis_v0.5_train, lvis_v0.5_val, lvis_v0.5_val_rand_100, lvis_v0.5_test, lvis_v0.5_train_cocofied, lvis_v0.5_val_cocofied, cityscapes_fine_instance_seg_train, cityscapes_fine_sem_seg_train, cityscapes_fine_instance_seg_val, cityscapes_fine_sem_seg_val, cityscapes_fine_instance_seg_test, cityscapes_fine_sem_seg_test, cityscapes_fine_panoptic_train, cityscapes_fine_panoptic_val, voc_2007_trainval, voc_2007_train, voc_2007_val, voc_2007_test, voc_2012_trainval, voc_2012_train, voc_2012_val, ade20k_sem_seg_train, ade20k_sem_seg_val)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DatasetCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f6ae8c-1313-4b3d-84df-2ef67f82c54a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d019695c-b275-418c-a711-87deb2f125b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda422ad-5d37-4379-ab90-539f78a081c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3925edc4-58ee-4ec3-a68f-7fd1e9b24b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c86c67-e5a6-4a27-a23e-48e1dfbcadce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "264f9c20-ca81-4317-bbd8-1c5f320c059c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(name='yeastcity_train')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MetadataCatalog.get(\"yeastcity_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32d046a-9d78-4178-840d-2e52707fa873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776fbd02-ad51-426d-ad3e-37de0fb90b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf1db2-4170-43fb-a890-ed38627246f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d147d8-2825-4289-8d53-1f91b73e0e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c5ccef-ba36-4e9d-b6c4-f5ef1452b04b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60226bce-3070-40f1-b0bf-7c13d466f76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5b0e52-7f55-46c9-9d10-594ff450801f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2f5b13-44c4-4b8e-8de0-982d1b971439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec429ce-a369-40dc-85ff-a4bd51e06ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "593b1fba-e83c-47ca-905b-573a8167147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "\n",
    "\"\"\"\n",
    "Panoptic-DeepLab Training Script.\n",
    "This script is a simplified version of the training script in detectron2/tools.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import MetadataCatalog, build_detection_train_loader\n",
    "from detectron2.engine import DefaultTrainer, default_argument_parser, default_setup, launch\n",
    "from detectron2.evaluation import (\n",
    "    CityscapesInstanceEvaluator,\n",
    "    CityscapesSemSegEvaluator,\n",
    "    COCOEvaluator,\n",
    "    COCOPanopticEvaluator,\n",
    "    DatasetEvaluators,\n",
    ")\n",
    "from detectron2.projects.deeplab import build_lr_scheduler\n",
    "from detectron2.projects.panoptic_deeplab import (\n",
    "    PanopticDeeplabDatasetMapper,\n",
    "    add_panoptic_deeplab_config,\n",
    ")\n",
    "from detectron2.solver import get_default_optimizer_params\n",
    "from detectron2.solver.build import maybe_add_gradient_clipping\n",
    "\n",
    "from detectron2.utils.events import CommonMetricPrinter, JSONWriter\n",
    "sys.path.insert(0, (\"/\").join(os.path.abspath('.').split(\"/\")[:-3]))\n",
    "from oneformer.utils.events import WandbWriter, setup_wandb\n",
    "\n",
    "def build_sem_seg_train_aug(cfg):\n",
    "    augs = [\n",
    "        T.RescaleShortestEdge(\n",
    "            cfg.INPUT.MIN_SIZE_TRAIN,\n",
    "            cfg.INPUT.MAX_SIZE_TRAIN,\n",
    "            cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING,\n",
    "        )\n",
    "    ]\n",
    "    # if cfg.INPUT.CROP.ENABLED:\n",
    "    #     augs.append(T.RandomCrop(cfg.INPUT.CROP.TYPE, cfg.INPUT.CROP.SIZE))\n",
    "    # augs.append(T.RandomFlip())\n",
    "    # augs.append(T.RandomBrightness(intensity_min=0.5, intensity_max=1.5))\n",
    "    # augs.append(T.RandomContrast(intensity_min=0.5, intensity_max=1.5))\n",
    "    augs.append(T.RandomApply(T.RandomFlip(), 0.5))\n",
    "    augs.append(T.RandomApply(T.RandomBrightness(intensity_min=0.5, intensity_max=1.5), 0.5))\n",
    "    augs.append(T.RandomApply(T.RandomContrast(intensity_min=0.5, intensity_max=1.5), 0.5))\n",
    "    if cfg.INPUT.CROP.ENABLED:\n",
    "        augs.append(T.FixedSizeCrop(cfg.INPUT.CROP.SIZE))\n",
    "    return augs\n",
    "\n",
    "\n",
    "class Trainer(DefaultTrainer):\n",
    "    \"\"\"\n",
    "    We use the \"DefaultTrainer\" which contains a number pre-defined logic for\n",
    "    standard training workflow. They may not work for you, especially if you\n",
    "    are working on a new research project. In that case you can use the cleaner\n",
    "    \"SimpleTrainer\", or write your own training loop.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        \"\"\"\n",
    "        Create evaluator(s) for a given dataset.\n",
    "        This uses the special metadata \"evaluator_type\" associated with each builtin dataset.\n",
    "        For your own dataset, you can simply create an evaluator manually in your\n",
    "        script and do not have to worry about the hacky if-else logic here.\n",
    "        \"\"\"\n",
    "        if cfg.MODEL.PANOPTIC_DEEPLAB.BENCHMARK_NETWORK_SPEED:\n",
    "            return None\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        evaluator_list = []\n",
    "        evaluator_type = MetadataCatalog.get(dataset_name).evaluator_type\n",
    "        if evaluator_type in [\"cityscapes_panoptic_seg\", \"coco_panoptic_seg\", \"yeastcity_panoptic_seg\"]:\n",
    "            evaluator_list.append(COCOPanopticEvaluator(dataset_name, output_folder))\n",
    "        if evaluator_type == \"cityscapes_panoptic_seg\":\n",
    "            evaluator_list.append(CityscapesSemSegEvaluator(dataset_name))\n",
    "            evaluator_list.append(CityscapesInstanceEvaluator(dataset_name))\n",
    "        if evaluator_type == \"yeastcity_panoptic_seg\":\n",
    "            evaluator_list.append(CityscapesSemSegEvaluator(dataset_name))\n",
    "            evaluator_list.append(CityscapesInstanceEvaluator(dataset_name))\n",
    "        if evaluator_type == \"coco_panoptic_seg\":\n",
    "            # `thing_classes` in COCO panoptic metadata includes both thing and\n",
    "            # stuff classes for visualization. COCOEvaluator requires metadata\n",
    "            # which only contains thing classes, thus we map the name of\n",
    "            # panoptic datasets to their corresponding instance datasets.\n",
    "            dataset_name_mapper = {\n",
    "                \"coco_2017_val_panoptic\": \"coco_2017_val\",\n",
    "                \"coco_2017_val_100_panoptic\": \"coco_2017_val_100\",\n",
    "            }\n",
    "            evaluator_list.append(\n",
    "                COCOEvaluator(dataset_name_mapper[dataset_name], output_dir=output_folder)\n",
    "            )\n",
    "        if len(evaluator_list) == 0:\n",
    "            raise NotImplementedError(\n",
    "                \"no Evaluator for the dataset {} with the type {}\".format(\n",
    "                    dataset_name, evaluator_type\n",
    "                )\n",
    "            )\n",
    "        elif len(evaluator_list) == 1:\n",
    "            return evaluator_list[0]\n",
    "        return DatasetEvaluators(evaluator_list)\n",
    "\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        mapper = PanopticDeeplabDatasetMapper(cfg, augmentations=build_sem_seg_train_aug(cfg))\n",
    "        return build_detection_train_loader(cfg, mapper=mapper)\n",
    "\n",
    "    @classmethod\n",
    "    def build_lr_scheduler(cls, cfg, optimizer):\n",
    "        \"\"\"\n",
    "        It now calls :func:`detectron2.solver.build_lr_scheduler`.\n",
    "        Overwrite it if you'd like a different scheduler.\n",
    "        \"\"\"\n",
    "        return build_lr_scheduler(cfg, optimizer)\n",
    "\n",
    "    @classmethod\n",
    "    def build_optimizer(cls, cfg, model):\n",
    "        \"\"\"\n",
    "        Build an optimizer from config.\n",
    "        \"\"\"\n",
    "        params = get_default_optimizer_params(\n",
    "            model,\n",
    "            weight_decay=cfg.SOLVER.WEIGHT_DECAY,\n",
    "            weight_decay_norm=cfg.SOLVER.WEIGHT_DECAY_NORM,\n",
    "        )\n",
    "\n",
    "        optimizer_type = cfg.SOLVER.OPTIMIZER\n",
    "        if optimizer_type == \"SGD\":\n",
    "            return maybe_add_gradient_clipping(cfg, torch.optim.SGD)(\n",
    "                params,\n",
    "                cfg.SOLVER.BASE_LR,\n",
    "                momentum=cfg.SOLVER.MOMENTUM,\n",
    "                nesterov=cfg.SOLVER.NESTEROV,\n",
    "            )\n",
    "        elif optimizer_type == \"ADAM\":\n",
    "            return maybe_add_gradient_clipping(cfg, torch.optim.Adam)(params, cfg.SOLVER.BASE_LR)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"no optimizer type {optimizer_type}\")\n",
    "\n",
    "    def build_writers(self):\n",
    "        \"\"\"\n",
    "        Build a list of writers to be used. By default it contains\n",
    "        writers that write metrics to the screen,\n",
    "        a json file, and a tensorboard event file respectively.\n",
    "        If you'd like a different list of writers, you can overwrite it in\n",
    "        your trainer.\n",
    "        Returns:\n",
    "            list[EventWriter]: a list of :class:`EventWriter` objects.\n",
    "        It is now implemented by:\n",
    "        ::\n",
    "            return [\n",
    "                CommonMetricPrinter(self.max_iter),\n",
    "                JSONWriter(os.path.join(self.cfg.OUTPUT_DIR, \"metrics.json\")),\n",
    "                TensorboardXWriter(self.cfg.OUTPUT_DIR),\n",
    "            ]\n",
    "        \"\"\"\n",
    "        # Here the default print/log frequency of each writer is used.\n",
    "        return [\n",
    "            # It may not always print what you want to see, since it prints \"common\" metrics only.\n",
    "            CommonMetricPrinter(self.max_iter),\n",
    "            JSONWriter(os.path.join(self.cfg.OUTPUT_DIR, \"metrics.json\")),\n",
    "            WandbWriter(),\n",
    "        ]\n",
    "\n",
    "def setup(args):\n",
    "    \"\"\"\n",
    "    Create configs and perform basic setups.\n",
    "    \"\"\"\n",
    "    cfg = get_cfg()\n",
    "    add_panoptic_deeplab_config(cfg)\n",
    "    cfg.merge_from_file(args.config_file)\n",
    "    cfg.merge_from_list(args.opts)\n",
    "    cfg.freeze()\n",
    "    default_setup(cfg, args)\n",
    "    setup_wandb(cfg, args)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    cfg = setup(args)\n",
    "\n",
    "    if args.eval_only:\n",
    "        model = Trainer.build_model(cfg)\n",
    "        DetectionCheckpointer(model, save_dir=cfg.OUTPUT_DIR).resume_or_load(\n",
    "            cfg.MODEL.WEIGHTS, resume=args.resume\n",
    "        )\n",
    "        res = Trainer.test(cfg, model)\n",
    "        return res\n",
    "\n",
    "    trainer = Trainer(cfg)\n",
    "    trainer.resume_or_load(resume=args.resume)\n",
    "    return trainer.train()\n",
    "\n",
    "\n",
    "def invoke_main() -> None:\n",
    "    args = default_argument_parser().parse_args()\n",
    "    print(\"Command Line Args:\", args)\n",
    "    launch(\n",
    "        main,\n",
    "        args.num_gpus,\n",
    "        num_machines=args.num_machines,\n",
    "        machine_rank=args.machine_rank,\n",
    "        dist_url=args.dist_url,\n",
    "        args=(args,),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5be5054a-0f5e-41d0-8866-c0e090296e73",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Config file '/data/martin/liwa/PytrochDeepyeast/projects/Panoptic-DeepLab/projects/Panoptic-DeepLab/configs/yeast_panoptics/city_deeplab_crop_512x512_300data.yaml' does not exist!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg \u001b[38;5;241m=\u001b[39m get_cfg()\n\u001b[1;32m     20\u001b[0m add_panoptic_deeplab_config(cfg)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# cfg.merge_from_list(args.opts)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m cfg\u001b[38;5;241m.\u001b[39mfreeze()\n",
      "File \u001b[0;32m/data/martin/liwa/PytrochDeepyeast/detectron2/config/config.py:45\u001b[0m, in \u001b[0;36mCfgNode.merge_from_file\u001b[0;34m(self, cfg_filename, allow_unsafe)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge_from_file\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg_filename: \u001b[38;5;28mstr\u001b[39m, allow_unsafe: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    Load content from the given config file and merge it into self.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m        allow_unsafe: allow unsafe yaml syntax\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m PathManager\u001b[38;5;241m.\u001b[39misfile(cfg_filename), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m     loaded_cfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_yaml_with_base(cfg_filename, allow_unsafe\u001b[38;5;241m=\u001b[39mallow_unsafe)\n\u001b[1;32m     47\u001b[0m     loaded_cfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(loaded_cfg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Config file '/data/martin/liwa/PytrochDeepyeast/projects/Panoptic-DeepLab/projects/Panoptic-DeepLab/configs/yeast_panoptics/city_deeplab_crop_512x512_300data.yaml' does not exist!"
     ]
    }
   ],
   "source": [
    "# cfg = setup(args)\n",
    "\n",
    "# if args.eval_only:\n",
    "#     model = Trainer.build_model(cfg)\n",
    "#     DetectionCheckpointer(model, save_dir=cfg.OUTPUT_DIR).resume_or_load(\n",
    "#         cfg.MODEL.WEIGHTS, resume=args.resume\n",
    "#     )\n",
    "#     res = Trainer.test(cfg, model)\n",
    "#     return res\n",
    "config_path = os.path.abspath(\"./projects/Panoptic-DeepLab/configs/yeast_panoptics/city_deeplab_crop_512x512_300data.yaml\")\n",
    "\n",
    "args = {\"num_gpus\": 4,\n",
    "       \"config_file\":config_path}\n",
    "#  --dist-url 'tcp://127.0.0.1:50164' \\\n",
    "# --config-file $config_path \\\n",
    "# --num-gpus 4\n",
    "\n",
    "\n",
    "cfg = get_cfg()\n",
    "add_panoptic_deeplab_config(cfg)\n",
    "cfg.merge_from_file(config_path)\n",
    "# cfg.merge_from_list(args.opts)\n",
    "cfg.freeze()\n",
    "default_setup(cfg, args)\n",
    "setup_wandb(cfg, args)\n",
    "trainer = Trainer(cfg)\n",
    "trainer.resume_or_load()\n",
    "# return trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c021e2a-ba3b-48b9-8851-0f072aa38756",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43margs\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04064ba0-81fd-4971-b1a0-88d57bf3f557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
